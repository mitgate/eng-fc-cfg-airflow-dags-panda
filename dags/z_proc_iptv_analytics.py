# Nome da DAG: z_proc_iptv_analytics
# Owner / responsÃ¡vel: Leandro
# DescriÃ§Ã£o do objetivo da DAG: Sem descriÃ§Ã£o detalhada
# Usa Druid?: NÃ£o
# Principais tabelas / consultas Druid acessadas: 
# FrequÃªncia de execuÃ§Ã£o (schedule): 
# Dag Activo?: 
# Autor: Leandro
# Data de modificaÃ§Ã£o: 2025-05-26

# v0.3
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.exceptions import AirflowSkipException
from airflow.operators.dummy import DummyOperator
from airflow.models import Variable
from airflow.utils.dates import datetime, timedelta
from hooks.kafka_connector import KafkaConnector
from base64 import b64encode
from operators.iptv_operator import IptvConnector


TOPIC= "procc_iptv_topic" 
DATE_FORMAT = "%Y-%m-%dT%H:%M:%SZ"
VAR_TIME ='datetime_proc_iptv_analytics'
DAG_NAME = 'Proc_IPTV_analytics'
INDEX = "analytics"


dag = DAG(
    DAG_NAME,
    default_args={
        'owner': 'Leandro',
        'depends_on_past': False,
        'retries': 0,
        'retry_delay': timedelta(minutes=5),
    },
    description=f'DAG para processar dados de IPTV ({INDEX})',
    schedule_interval= None, #'*/5 * * * *',  # Executa a cada 30 minutos
    start_date=datetime(2024, 12, 9),
    catchup=False,
    max_active_runs=1,
    tags=['processamento', 'kafka', 'iptv'],
)

def set_airflow_variable(variable_name, value):
    Variable.set(variable_name, value)

# Obtendo uma variÃ¡vel de data
def get_airflow_variable(variable_name, default_value=None):
    return Variable.get(variable_name, default_var=default_value)

def main():
    string_past_24_hour = (datetime.now() - timedelta(hours=24)).strftime(DATE_FORMAT)
    var_datetime_max = VAR_TIME 
    print(f'ğŸ•“ Start date if the variable {var_datetime_max} is not found: {string_past_24_hour}')
    start_date = get_airflow_variable(var_datetime_max,string_past_24_hour)
    
    if start_date is not None:
        print(f'âœ…ğŸ•“ Variable found, value filtered after: {start_date}')
    else:
         print(f'ğŸŸ¡ğŸ•“ Variable not found, value filtered after: {start_date}') 
    
    print(f'ğŸŸ£Create connection from IPTV using this index: {INDEX}.')
    iptv = IptvConnector(INDEX)
    
    result, max_datetime = iptv.main(start_date)
    
    if len(result)>0:

        print('ğŸ”µ Start connection with Kafka.')
        kafka = KafkaConnector(
            topic_var_name=TOPIC,
            kafka_url_var_name="prod_kafka_url",
            kafka_port_var_name="prod_kafka_port",
            kafka_variable_pem_content="pem_content",
            kafka_connection_id="kafka_default"
        )
        producer = kafka.create_kafka_connection()
        print('ğŸ”µ Created Kafka producer.')

        print('ğŸ”µ Sending data to Kafka...')
        kafka.send_mult_messages_to_kafka(  
            menssages=result,
            producer=producer,
            key=INDEX
        )
        print(f'âœ… Messages sent on Kafka.')
        
        set_airflow_variable(var_datetime_max,max_datetime)
        print(f"ğŸ”¼ Variable {var_datetime_max} updated with new datetime: {max_datetime}")
    else:
        print('âœ… No records returned. Process finished!')
        raise AirflowSkipException('âœ… No records returned. Process finished!')

start = DummyOperator(
    task_id='start',
    dag=dag
)

process_counters = PythonOperator(
    task_id='process_iptv',
    python_callable=main,
    provide_context=True,
    execution_timeout=timedelta(minutes=20),  # Limita a execuÃ§Ã£o da task a 20 minutos
    dag=dag
)

end = DummyOperator(
    task_id='end',
    dag=dag
)

# Definindo as dependÃªncias das tarefas
start >> process_counters >> end
