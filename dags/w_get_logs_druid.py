# Nome da DAG: w_get_logs_druid
# Owner / respons√°vel: Sadir
# Descri√ß√£o do objetivo da DAG: DAG para coletar os logs do Druid
# Usa Druid?: Sim
# Principais tabelas / consultas Druid acessadas: 
# Frequ√™ncia de execu√ß√£o (schedule): */5 * * * *
# Dag Activo?: 
# Autor: Sadir
# Data de modifica√ß√£o: 2025-05-26

# 2

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator
from airflow.models import Variable
from airflow.utils.dates import datetime, timedelta
import os
import json
from airflow.exceptions import AirflowSkipException
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
import tempfile
from confluent_kafka import Producer
import sys
import ssl
import subprocess

# Definindo argumentos padr√£o da DAG
default_args = {
    'owner': 'Sadir',
    'depends_on_past': False,
    'retries': 0,
    'retry_delay': timedelta(minutes=10),
}

# Definindo a DAG explicitamente
dag = DAG(
    dag_id='Get_Logs_Druid',
    default_args=default_args,
    description='DAG para coletar os logs do Druid',
    schedule_interval='*/5 * * * *',  # Executa a cada 15 minutos
    start_date=datetime(2024, 9, 9),
    catchup=False,
    max_active_runs=1,  # Para garantir que apenas uma execu√ß√£o da DAG ocorra por vez
    tags=['logs', 'druid'],
)

# Fun√ß√µes Python
#=================================================================================================


user = Variable.get("logs_druid_user_open_shift") 
password = Variable.get("logs_druid_password_open_shift") 
host = "https://api.ocp-01.tdigital-vivo.com.br:6443"
namespace = "panda-druid"
log_dir = "/druid/data/sql/"  # Diret√≥rio onde os arquivos de logs est√£o no pod 
DEFAULT_TOPIC = Variable.get("logs_druid_topic") #"druid-test"
TIME_FILTER_MINUTES= int(Variable.get("logs_druid_time_filter")) #10
KAFKA_URL = "amqstreams-kafka-external-bootstrap-panda-amq-streams-dev.apps.ocp-01.tdigital-vivo.com.br"
#KAFKA_URL = "amqstreams-kafka-external-bootstrap.panda-amq-streams-dev.svc.cluster.local"
KAFKA_PORT = "443"
#KAFKA_PORT = "9095"



recent_time_threshold = (datetime.now() ) - timedelta(minutes=TIME_FILTER_MINUTES)
print(f"‚è∞ Hora base do filtro {recent_time_threshold}")


# Funci√≥n para manejar la entrega de mensajes
def delivery_callback(err, msg):
    if err:
        print('‚ùå Error al enviar mensaje: %s' % err)
        raise

# Funci√≥n para leer un archivo l√≠nea por l√≠nea y publicar cada l√≠nea en Kafka
def publish_file_to_kafka(logs):    
    pem_content = Variable.get("pem_content_dev")

    with tempfile.NamedTemporaryFile(delete=False) as temp_file:
        temp_file.write(pem_content.encode())

    ssl_location = temp_file.name
            
    # Configura√ß√µes do Kafka

    
    conf = {
        "bootstrap.servers": f"{KAFKA_URL}:{KAFKA_PORT}",
        "security.protocol": "SASL_SSL",
        "sasl.mechanism": "SCRAM-SHA-512",
        "sasl.username": "flink",
        "sasl.password": "NMoW680oSNitVesBQti9jlsjl7GC8u36",
        'ssl.ca.location': ssl_location,
        'message.max.bytes': '1000000000',
        'batch.num.messages': 1000000,  # Aumentar para 500 mil ou mais
        'linger.ms': 50,  # Aumentar para 50ms
        'compression.type': 'lz4',  # Habilitar compress√£o
        'queue.buffering.max.messages': 2000000,  # Aumentar o tamanho do buffer
        'queue.buffering.max.kbytes': 2097152,  # Aumentar o limite em KB (1 GB)
        'max.in.flight.requests.per.connection': 5,  # Aumentar requisi√ß√µes simult√¢neas
        'queue.buffering.max.ms': 500, # Tempo m√°ximo para agrupar mensagens
        'message.send.max.retries': 5, # Retry na hora de enviar
        'retry.backoff.ms': 500, # Tempo entre cada retry
    }
    # Carregar certificados confi√°veis no contexto SSL
    ssl_context = ssl.create_default_context()
    ssl_context.load_verify_locations(conf['ssl.ca.location'])

    producer = Producer(**conf)
    print( "üîù Iniciando envio ao Kafka...")
    quantidade_de_logs = len(logs)
    print(f"üîù Total de logs: {quantidade_de_logs}")
    contador = 1
    try:
        for line in logs:
            nueva_linea = json.dumps(line, indent=2).encode('utf-8') #.replace('\t', ',').replace('\n', '')               
            producer.produce(DEFAULT_TOPIC, nueva_linea, callback=delivery_callback)
            print(f"‚ñ∂ Mensagem {contador} enviada.")
            contador += 1
                          
        producer.flush()  
    #    print("Todas las l√≠neas se han publicado en Kafka exitosamente.")
    except Exception as e:
        print("Error al publicar l√≠neas en Kafka:", e)
        
# Fun√ß√£o para executar comandos no terminal
def run_command(command):
    try:
        result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
        print(f"üî¥ Erro ao executar comando: {e}")
        return None

# Fun√ß√£o para fazer login no OpenShift
def oc_login(user, password, host):
    print("üé© Realizando login no OpenShift...")
    login_command = f"oc login --server={host} -u {user} -p {password} --insecure-skip-tls-verify=true"
    return run_command(login_command)

# Fun√ß√£o para listar os pods no namespace panda-druid
def get_pods(namespace):
    print(f"‚õè Obtendo pods no namespace {namespace}...")
    get_pods_command = f"oc get pods -n {namespace} --no-headers -o custom-columns=NAME:.metadata.name"
    return run_command(get_pods_command).splitlines()

# Fun√ß√£o para listar os arquivos de log dentro do diret√≥rio do pod
def list_log_files_in_pod(pod_name, namespace, log_dir):
    print(f"üìì Listando arquivos de log no diret√≥rio {log_dir} do pod {pod_name}...")
    list_files_command = f"oc exec {pod_name} -n {namespace} -- ls {log_dir}"
    files = run_command(list_files_command)
    if files:
        log_files = [file for file in files.splitlines() if file.endswith('.log')]
        return log_files
    else:
        print(f"‚ùå Falha ao listar arquivos no diret√≥rio {log_dir} do pod {pod_name}.")
        return []

# Fun√ß√£o para baixar o arquivo de log do pod e salvar em um arquivo tempor√°rio
def download_log_file_to_temp(pod_name, namespace, log_file_path):
    print(f"üîé Baixando o arquivo {log_file_path} do pod {pod_name} para um arquivo tempor√°rio...")
    logs_command = f"oc exec {pod_name} -n {namespace} -- cat {log_file_path}"
    logs = run_command(logs_command)
    if logs:
        # Criar um arquivo tempor√°rio para armazenar os logs
        temp_file = tempfile.NamedTemporaryFile(delete=False, mode='w')
        temp_file.write(logs)
        temp_file.close()
        return temp_file.name  # Retorna o caminho do arquivo tempor√°rio
    else:
        print(f"‚ùå Falha ao baixar logs do arquivo {log_file_path} no pod {pod_name}.")
        return None

# Fun√ß√£o para filtrar os logs dos √∫ltimos X minutos
def filter_recent_logs_from_file(file_path, minutes=TIME_FILTER_MINUTES):
    print(f"‚è∞ Filtrando logs dos √∫ltimos {minutes} minutos no arquivo {file_path}...")
    recent_logs = []

    # Ler os logs do arquivo tempor√°rio
    with open(file_path, 'r') as file:
        for line in file:
            # Extrair timestamp no in√≠cio da linha
            match = re.match(r"(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d{3}Z)", line)
            if match:
                log_time_str = match.group(1)
                log_time = datetime.strptime(log_time_str, "%Y-%m-%dT%H:%M:%S.%fZ")
                # Comparar com o tempo atual menos X minutos
                if log_time >= recent_time_threshold:
                    recent_logs.append(line)

    # Remover o arquivo tempor√°rio ap√≥s o uso
    os.remove(file_path)
    return recent_logs

# Fun√ß√£o que extrai as informa√ß√µes do log com base no padr√£o fornecido
def extract_log_info(line):
    fields = line.split("\t")

    log_data = {
        "timestamp": None,
        "queryType_json": None,
        "query_json": None,
        "sqlquery_time": None
    }

    # Extrair o timestamp (data e hora do evento) - sempre o primeiro campo
    if len(fields) > 0:
        log_data["timestamp"] = fields[0]

    # Extrair o JSON com "queryType" e "query"
    for field in fields:
        if field.startswith('{"queryType":'):
            try:
                ajustado = json.loads(field.encode('utf-8'))
                if 'intervals' in ajustado.keys():
                    if 'segments' in ajustado['intervals'].keys():
                        ajustado['intervals']['segments'] = len(ajustado['intervals']['segments'])
                log_data["queryType_json"] = ajustado
            except json.JSONDecodeError as e:
                print(f"‚ùé‚ùå Erro ao decodificar JSON em 'queryType': {e}")
                print(f"String com erro: {field[1:50]}")
                log_data["queryType_json"] = None  # Marcar como None se o JSON n√£o for v√°lido

        elif field.startswith('{"query":'):
            try:
                log_data["query_json"] = json.loads(field)
            except json.JSONDecodeError as e:
                print(f"‚ùé‚ùå Erro ao decodificar JSON em 'query': {e}")
                print(f"String com erro: {field[1:50]}")
                log_data["query_json"] = None  # Marcar como None se o JSON n√£o for v√°lido

        elif field.startswith('{"sqlQuery/time":'):
            try:
                log_data["sqlquery_time"] = json.loads(field)
            except json.JSONDecodeError as e:
                print(f"‚ùé‚ùå Erro ao decodificar JSON em 'sqlQuery/time': {e}")
                print(f"String com erro: {field[1:50]}")
                log_data["sqlquery_time"] = None  # Marcar como None se o JSON n√£o for v√°lido

    return log_data

# Fun√ß√£o que lida com a extra√ß√£o e filtragem de logs de um pod
def process_pod_logs(pod, namespace, log_dir, minutes=TIME_FILTER_MINUTES):
    all_recent_logs = []  # Lista para armazenar todos os logs recentes deste pod
    log_files = list_log_files_in_pod(pod, namespace, log_dir)
    for log_file in log_files:
        log_file_path = f"{log_dir}/{log_file}"
        temp_file = download_log_file_to_temp(pod, namespace, log_file_path)
        if temp_file:
            recent_logs = filter_recent_logs_from_file(temp_file, minutes=minutes)
            all_recent_logs.extend(recent_logs)
    return all_recent_logs

# Fun√ß√£o principal para orquestrar o processo completo com execu√ß√£o paralela
def extract_and_combine_recent_logs():

    # 1. Fazer login no OpenShift
    if oc_login(user, password, host) is None:
        pass
        #return

    # 2. Obter a lista de pods no namespace panda-druid
    pods = get_pods(namespace)
    if not pods:
        print("üòÖ Nenhum pod encontrado no namespace.")
        #return

    # 3. Filtrar pods do broker (presumindo que os nomes contenham 'broker')
    broker_pods = [pod for pod in pods if 'broker' in pod]
    if not broker_pods:
        print("üòÖ Nenhum pod do broker encontrado.")
        #return

    # 4. Processar logs de cada pod em paralelo
    all_recent_logs = []  # Lista para armazenar todos os logs recentes de todos os pods
    with ThreadPoolExecutor(max_workers=len(broker_pods)) as executor:
        future_to_pod = {executor.submit(process_pod_logs, pod, namespace, log_dir, TIME_FILTER_MINUTES): pod for pod in broker_pods}

        for future in as_completed(future_to_pod):
            pod = future_to_pod[future]
            try:
                recent_logs = future.result()
                if recent_logs:
                    all_recent_logs.extend(recent_logs)
            except Exception as exc:
                print(f"üî¥ Pod {pod} gerou uma exce√ß√£o: {exc}")

    logs_clean = sorted(set(all_recent_logs))

    logs_to_send = [extract_log_info(i) for i in logs_clean]

    publish_file_to_kafka(logs=logs_to_send)



#=================================================================================================
# Definindo as tasks da DAG
start = DummyOperator(
    task_id='start',
    dag=dag,
)

process_dashboards_task = PythonOperator(
    task_id='get_logs',
    python_callable=extract_and_combine_recent_logs,
    provide_context=True,
    execution_timeout=timedelta(minutes=30),  # Limita a execu√ß√£o da task a 20 minutos
    dag=dag,
)

end = DummyOperator(
    task_id='end',
    dag=dag,
)

# Definindo as depend√™ncias das tarefas
start >> process_dashboards_task >> end
